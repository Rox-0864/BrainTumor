Este bloque es el núcleo del proyecto, donde se define la arquitectura del modelo de Deep Learning utilizando Transfer Learning con ResNet50, se compila el modelo especificando el optimizador, la función de pérdida y las métricas, y finalmente, se lleva a cabo el proceso de entrenamiento en dos fases: primero entrenando solo las capas clasificadoras nuevas y luego realizando un ajuste fino (fine-tuning) de todo el modelo.

## 3.1 Construcción del Modelo ResNet50
Se define una función `build_resnet_model` para encapsular la creación del modelo.
1.  **Carga de ResNet50 Pre-entrenada**: Se carga la arquitectura ResNet50 con pesos pre-entrenados en ImageNet. Crucialmente, `include_top=False` se utiliza para descartar la capa clasificadora original de ResNet50, ya que la reemplazaremos con una adaptada a nuestra tarea binaria.
2.  **Congelación Inicial de la Base**: El modelo base ResNet50 (`base_model`) se establece inicialmente como no entrenable (`base_model.trainable = False`). Esto significa que sus pesos no se actualizarán durante la primera fase de entrenamiento.
3.  **Adición de Capas Clasificadoras Personalizadas**:
    *   Se toma la salida del `base_model` y se le aplica `GlobalAveragePooling2D` para reducir la dimensionalidad espacial de los mapas de características.
    *   Se añade una capa de `Dropout` para regularización y prevenir el sobreajuste.
    *   Finalmente, se añade una capa `Dense` con una sola neurona y activación `sigmoid` para la clasificación binaria. Se incluye regularización L2 en esta capa.
4.  **Creación del Modelo Final**: Se ensambla el modelo completo utilizando la API Funcional de Keras, especificando las entradas y salidas.

```python
# %% Construcción del modelo ResNet50
def build_resnet_model(input_shape=(224, 224, 3)):
    # Cargar ResNet50 preentrenada (sin las capas superiores de clasificación de ImageNet)
    base_model = ResNet50(
        include_top=False,      # No incluir la capa densa final de ResNet50
        weights='imagenet',     # Usar pesos pre-entrenados en ImageNet
        input_shape=input_shape # Definir la forma de entrada de las imágenes
    )

    # Congelar las capas del modelo base inicialmente.
    # Sus pesos no se actualizarán durante la primera fase de entrenamiento.
    base_model.trainable = False

    # Construir el modelo completo añadiendo nuestras propias capas encima de ResNet50
    inputs = layers.Input(shape=input_shape) # Capa de entrada
    # Pasar las entradas a través del modelo base.
    x = base_model(inputs, training=False)
    # Reducir la dimensionalidad espacial a un vector por cada mapa de características.
    x = layers.GlobalAveragePooling2D()(x)
    x = layers.Dropout(0.5)(x)  # Capa de Dropout para regularización
    # Capa densa final para la clasificación binaria, con activación sigmoide.
    # Se añade regularización L2 al kernel para prevenir el sobreajuste.
    outputs = layers.Dense(1, activation='sigmoid',
                          kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)

    # Crear el modelo final especificando las entradas y salidas.
    model = Model(inputs, outputs)
    return model

# Instanciar el modelo
model = build_resnet_model()
```

## 3.2 Compilación del Modelo
Antes de entrenar, el modelo debe ser compilado. La compilación configura el proceso de aprendizaje.
- **Optimizador:** Se utiliza Adam con una tasa de aprendizaje inicial (ej. 1e-3). Adam es un optimizador eficiente y popular.
- **Función de Pérdida:** Para clasificación binaria con una salida sigmoide, se usa binary_crossentropy.
- **Métricas:** Se monitorean la accuracy (exactitud) y el AUC (Área Bajo la Curva ROC) durante el entrenamiento y la evaluación.

```python
# Compilación del modelo
model.compile(
    optimizer=Adam(learning_rate=1e-3), # Optimizador Adam con una tasa de aprendizaje inicial
    loss='binary_crossentropy',         # Función de pérdida para clasificación binaria
    metrics=['accuracy', AUC(name='auc')] # Métricas a monitorear
)
```

## 3.3 Definición de Callbacks
Los callbacks son funciones que se ejecutan en diferentes puntos durante el entrenamiento para realizar acciones como detener el entrenamiento prematuramente, ajustar la tasa de aprendizaje o guardar el mejor modelo.
- **EarlyStopping:** Detiene el entrenamiento si la métrica monitoreada (val_auc) no mejora después de un número determinado de épocas (patience). restore_best_weights=True asegura que los pesos del modelo se reviertan a los de la mejor época.
- **ReduceLROnPlateau:** Reduce la tasa de aprendizaje si la métrica monitoreada (val_loss) se estanca.
- **ModelCheckpoint:** Guarda el modelo (o solo sus pesos) en un archivo cada vez que la métrica monitoreada (val_auc) mejora. save_best_only=True asegura que solo se guarde el mejor modelo.

```python
# Callbacks para el entrenamiento
callbacks_list = [
    callbacks.EarlyStopping(
        monitor='val_auc',        # Métrica a monitorear (AUC en el conjunto de validación)
        patience=5,               # Número de épocas a esperar sin mejora antes de detener
        mode='max',               # Indica que buscamos maximizar la métrica (AUC)
        restore_best_weights=True,# Restaura los pesos del modelo de la mejor época al finalizar
        verbose=1                 # Muestra mensajes cuando el callback se activa
    ),
    callbacks.ReduceLROnPlateau(
        monitor='val_loss',       # Métrica a monitorear (pérdida en el conjunto de validación)
        factor=0.1,               # Factor por el cual se reduce la tasa de aprendizaje (new_lr = lr * factor)
        patience=3,               # Número de épocas a esperar sin mejora antes de reducir LR
        verbose=1
    ),
    callbacks.ModelCheckpoint(
        'best_resnet_model.h5',   # Nombre del archivo para guardar el mejor modelo
        monitor='val_auc',        # Métrica que determina si el modelo es "mejor"
        save_best_only=True,      # Guarda solo el modelo si la métrica monitoreada ha mejorado
        mode='max',               # El objetivo es maximizar val_auc
        verbose=1
    )
]
```

## 3.4 Entrenamiento en Dos Fases
El entrenamiento se realiza en dos fases para aprovechar eficazmente el Transfer Learning.

### Fase 1: Entrenar Solo las Capas Nuevas (Clasificador Superior)
En esta fase, el modelo base ResNet50 permanece congelado (base_model.trainable = False), y solo se entrenan los pesos de las capas clasificadoras que se añadieron (GlobalAveragePooling2D, Dropout y Dense). Esto permite que el nuevo clasificador aprenda a interpretar las características extraídas por ResNet50 sin alterar los pesos pre-entrenados.

```python
# Fase 1: Entrenar solo las capas nuevas
print("\nEntrenando capas nuevas...")
initial_epochs = 10 # Número de épocas para la primera fase de entrenamiento
history = model.fit(
    train_gen,
    validation_data=valid_gen,
    epochs=initial_epochs,
    callbacks=callbacks_list, # Utilizar la lista de callbacks definida
    verbose=1                 # Mostrar barra de progreso e información por época
)
```

### Fase 2: Fine-Tuning de Todo el Modelo
Después de que las capas superiores se han estabilizado, se procede al ajuste fino (fine-tuning).
- **Descongelación del Modelo Base:** Se establece base_model.trainable = True, permitiendo que los pesos de ResNet50 también se actualicen.
- **Recompilación con Tasa de Aprendizaje Baja:** Es crucial recompilar el modelo después de cambiar el estado trainable de las capas. Para el fine-tuning, se utiliza una tasa de aprendizaje mucho más baja (ej. 1e-5) para evitar destruir las características pre-entrenadas y permitir solo ajustes sutiles.
- **Continuación del Entrenamiento:** El entrenamiento continúa por un número adicional de épocas.

```python
# Fase 2: Fine-tuning de todo el modelo
print("\nFine-tuning de todo el modelo...")
# Acceder a la capa base ResNet50 dentro del modelo 'model'
base_model_layer_from_model = model.layers[1] # Obtenemos la referencia a la capa ResNet50
base_model_layer_from_model.trainable = True   # Hacemos que la capa ResNet50 sea entrenable

# Recompilar el modelo con una tasa de aprendizaje mucho más baja para el fine-tuning
# Esto es esencial para no destruir los pesos pre-entrenados de ResNet50.
model.compile(
    optimizer=Adam(learning_rate=1e-5), # Tasa de aprendizaje significativamente menor
    loss='binary_crossentropy',
    metrics=['accuracy', AUC(name='auc')]
)

fine_tune_epochs = 10 # Número de épocas adicionales para el fine-tuning
total_epochs = initial_epochs + fine_tune_epochs # Número total de épocas de entrenamiento

# Continuar el entrenamiento (fine-tuning)
history_fine = model.fit(
    train_gen,
    validation_data=valid_gen,
    initial_epoch=history.epoch[-1]+1, # Comenzar el conteo de épocas desde el final de la fase anterior
    epochs=total_epochs,            # Entrenar hasta alcanzar el número total de épocas
    callbacks=callbacks_list,
    verbose=1
)
```

Al finalizar este bloque, el modelo model ha sido entrenado completamente. La variable history contiene el historial de entrenamiento de la primera fase, y history_fine el de la segunda fase (fine-tuning). El archivo best_resnet_model.h5 contendrá los pesos del mejor modelo encontrado durante todo el proceso de entrenamiento, según la métrica val_auc.
