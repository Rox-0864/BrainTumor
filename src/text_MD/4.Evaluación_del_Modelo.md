Una vez que el modelo ResNet50 ha sido entrenado mediante la estrategia de dos fases (entrenamiento del clasificador y fine-tuning), el siguiente paso crucial es evaluar su rendimiento en un conjunto de datos que no ha visto durante el entrenamiento: el conjunto de prueba. Además, se visualizan diversas métricas y el historial de entrenamiento para obtener una comprensión completa del comportamiento y la eficacia del modelo.

## 4.1 Evaluación del Modelo en el Conjunto de Prueba
Se define una función `evaluate_model` para realizar una evaluación sistemática.
1.  **Reinicio del Generador de Prueba**: `test_gen.reset()` es importante para asegurar que el generador comience desde la primera imagen del conjunto de prueba, garantizando la consistencia entre las predicciones y las etiquetas verdaderas.
2.  **Cálculo de Métricas**: Se utiliza `model.evaluate()` sobre el `test_gen` para obtener la pérdida (loss), la exactitud (accuracy) y el AUC en el conjunto de prueba.
3.  **Obtención de Predicciones**: Se generan las probabilidades predichas por el modelo (`y_pred_probs`) y luego se convierten a clases binarias (`y_pred`) usando un umbral de 0.5. Las etiquetas verdaderas (`y_true`) se obtienen de `test_gen.classes`.
4.  **Matriz de Confusión**: Se calcula y visualiza la matriz de confusión para ver el rendimiento del modelo en términos de verdaderos positivos, falsos positivos, verdaderos negativos y falsos negativos.
5.  **Reporte de Clasificación**: Se imprime un reporte de clasificación que incluye precisión, recall (sensibilidad), F1-score y soporte para cada clase.
6.  **Curva ROC y AUC**: Se calcula y grafica la curva ROC (Receiver Operating Characteristic) y el área bajo esta curva (AUC) para evaluar la capacidad del modelo para distinguir entre las clases a diferentes umbrales.

```python
# %% Evaluación del modelo
def evaluate_model(model, test_gen):
    # Reiniciar el generador de prueba para asegurar que empieza desde el principio
    test_gen.reset()
    # Evaluar el modelo en el conjunto de prueba
    loss, accuracy, auc_score = model.evaluate(test_gen, verbose=0)

    print(f"\nResultados en conjunto de prueba:")
    print(f"Pérdida (Loss): {loss:.4f}")
    print(f"Exactitud (Accuracy): {accuracy:.4f}")
    print(f"AUC: {auc_score:.4f}")

    # Obtener las probabilidades predichas por el modelo para el conjunto de prueba
    y_pred_probs = model.predict(test_gen)
    # Convertir las probabilidades a predicciones de clase (0 o 1) usando un umbral de 0.5
    y_pred = (y_pred_probs > 0.5).astype(int).flatten()
    # Obtener las etiquetas verdaderas del generador de prueba
    y_true = test_gen.classes

    # Obtener los nombres originales de las clases usando el label_encoder ajustado previamente
    class_names = list(label_encoder.inverse_transform([0, 1]))

    # --- Matriz de Confusión ---
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(6, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names)
    plt.title('Matriz de Confusión - ResNet50')
    plt.ylabel('Etiqueta Verdadera')
    plt.xlabel('Etiqueta Predicha')
    plt.show()

    # --- Reporte de Clasificación ---
    print("\nReporte de Clasificación:")
    print(classification_report(y_true, y_pred, target_names=class_names))

    # --- Curva ROC ---
    # Calcular la tasa de falsos positivos (fpr) y la tasa de verdaderos positivos (tpr)
    fpr, tpr, _ = roc_curve(y_true, y_pred_probs) # Usar y_pred_probs para la curva ROC
    # Calcular el Área Bajo la Curva ROC
    roc_auc = auc(fpr, tpr)

    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2,
             label=f'Curva ROC (área = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--') # Línea de no discriminación
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('Tasa de Falsos Positivos (FPR)')
    plt.ylabel('Tasa de Verdaderos Positivos (TPR)')
    plt.title('Curva ROC - ResNet50')
    plt.legend(loc="lower right")
    plt.grid(True)
    plt.show()

# Llamar a la función de evaluación con el modelo entrenado y el generador de prueba
evaluate_model(model, test_gen)
```

## 4.2 Visualización del Historial de Entrenamiento
Para entender cómo progresó el aprendizaje del modelo a lo largo de las épocas, se grafica el historial de entrenamiento. Esto incluye la exactitud y la pérdida tanto en el conjunto de entrenamiento como en el de validación para ambas fases del entrenamiento (inicial y fine-tuning).
La función plot_combined_history toma los objetos history (de la Fase 1) e history_fine (de la Fase 2) y combina sus métricas para una visualización continua. Una línea vertical indica dónde comenzó la fase de fine-tuning.

```python
# %% Visualización del historial de entrenamiento
def plot_combined_history(initial_hist, fine_tune_hist):
    # Extraer métricas de la fase inicial
    acc = initial_hist.history['accuracy']
    val_acc = initial_hist.history['val_accuracy']
    loss = initial_hist.history['loss']
    val_loss = initial_hist.history['val_loss']

    # Añadir métricas de la fase de fine-tuning
    acc += fine_tune_hist.history['accuracy']
    val_acc += fine_tune_hist.history['val_accuracy']
    loss += fine_tune_hist.history['loss']
    val_loss += fine_tune_hist.history['val_loss']

    plt.figure(figsize=(12, 6))

    # Subgráfico para la Exactitud
    plt.subplot(1, 2, 1)
    plt.plot(acc, label='Exactitud de Entrenamiento')
    plt.plot(val_acc, label='Exactitud de Validación')
    # Línea vertical para marcar el inicio del fine-tuning
    # Asumiendo que initial_epochs fue definido como en el script original
    plt.plot([initial_epochs-1, initial_epochs-1],
             plt.ylim(), label='Inicio Fine-Tuning', linestyle='--')
    plt.title('Exactitud durante Entrenamiento')
    plt.xlabel('Época')
    plt.ylabel('Exactitud')
    plt.legend()
    plt.grid(True)

    # Subgráfico para la Pérdida
    plt.subplot(1, 2, 2)
    plt.plot(loss, label='Pérdida de Entrenamiento')
    plt.plot(val_loss, label='Pérdida de Validación')
    # Línea vertical para marcar el inicio del fine-tuning
    plt.plot([initial_epochs-1, initial_epochs-1],
             plt.ylim(), label='Inicio Fine-Tuning', linestyle='--')
    plt.title('Pérdida durante Entrenamiento')
    plt.xlabel('Época')
    plt.ylabel('Pérdida')
    plt.legend()
    plt.grid(True)

    plt.tight_layout() # Ajustar el layout para evitar superposiciones
    plt.show()

# Llamar a la función para graficar el historial combinado
plot_combined_history(history, history_fine)
```

Este bloque finaliza el flujo de trabajo del modelo, proporcionando una evaluación cuantitativa y cualitativa de su rendimiento. Las visualizaciones ayudan a identificar posibles problemas como el sobreajuste o el subajuste, y las métricas en el conjunto de prueba ofrecen una estimación de cómo el modelo podría comportarse con datos nuevos y no vistos.
